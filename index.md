---
layout: default
---




Building on the documentation frameworks for "model cards" and "datasheets" proposed by Mitchell et al. and Gebru et al., we argue the need for Reward Reports for AI systems. 
In a whitepaper recently published by the Center for Long-Term Cybersecurity, we introduced Reward Reports as living documents for proposed RL deployments that demarcate design choices. 
However, many questions remain about the applicability of this framework to different RL applications, roadblocks to system interpretability, and the resonances between deployed supervised machine learning systems and the sequential decision-making utilized in RL. 
At a minimum, Reward Reports are an opportunity for RL practitioners to deliberate on these questions and begin the work of deciding how to resolve them in practice.

To learn more about Reward Reports, see the 
          <a href="https://arxiv.org/abs/2204.10817"> Reward Reports paper</a>, the
          <a href="http://arxiv.org/abs/2202.05716"> CLTC RL Risks Whitepaper</a> , or the 
          <a href="https://github.com/RewardReports/reward-reports">github repo</a> with a template, contribution guide, and more!

