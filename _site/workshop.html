<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Reward Reports for Reinforcement Learning</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.0/jquery.min.js"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="assets/css/base.css" rel="stylesheet">
  <link href="assets/css/style.css" rel="stylesheet">

  <link rel="shortcut icon" type="image/png" href="/assets/img/rewardreports2.png">
  <link rel="shortcut icon" sizes="192x192" href="/assets/img/rewardreports2.png">
  <link rel="apple-touch-icon" href="/assets/img/rewardreports2.png">
</head>

<nav class="navbar navbar-expand-lg navbar-light bg-light" id="navbar">

  <div class="container" id="header">

    <div>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-main" aria-controls="navbar-main" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
    </div> <!-- navbar-header -->

    <div class="collapse navbar-collapse" id="navbar-main">
      <ul class="navbar-nav mx-auto">
        <li class="nav-link"><a href="/">Home</a></li>
        <!-- <li class="nav-link"><a href="/speakers.html">Speakers</a></li> -->
        <li class="nav-link"><a href="/workshop.html">Workshop</a></li>
        <li class="nav-link"><a href="https://github.com/RewardReports/reward-reports">R.R. Template</a></li>
        <!-- <li class="nav-link"><a href="/papers.html">Accepted Papers</a></li> -->
        <li class="nav-link"><a href="/designers.html">Designers</a></li>
        <li class="nav-link"><a href="/related-work.html">Related Work</a></li>
      </ul>

    </div> <!-- navbar-main -->

  </div> 

</nav> <!-- navbar -->

<body>

  <div class="container" id="content">

    <div class="page-content">

      <div class="row justify-content-center">
        <img src=assets/img/rewardreports2.png class="img-fluid" style="max-width:300px" />
        <div class="break"></div>
        <h1 class="text-center">Reward Reports for Reinforcement Learning</h1>
        <div class="break"></div>
        <h3 class="text-center"><em>Towards Documentation and Understanding of Dynamic Machine Learning Systems</em></h3>
        <div class="break"></div>
        <h4 class="text-center"> <strong> <span class="important">Join us at our workshop </span>on Building Accountable and Transparent RL</strong>, at the <a href="https://rldm.org"> The Multi-disciplinary Conference on Reinforcement Learning and Decision Making
          (RLDM)</a> June 11th, 2022</h4>
        <div class="break"></div>
        <h4 class="text-center"> To learn more about Reward Reports, see the 
          <a href="/assets/reward_reports_for_rl.pdf"> Reward Reports paper</a>, the
          <a href="http://arxiv.org/abs/2202.05716"> CLTC RL Risks Whitepaper</a> , or the 
          <a href="https://github.com/RewardReports/reward-reports">github repo</a> with a template.
        </h4>
        <!-- <h4 class="text-center"></h4> -->
        <hr>

      </div> <!-- row -->

      <p><strong>The RLDM 2022  Workshop is on Saturday June 11th from 1:00-5:00pm EST.</strong></p>

<p><em>Workshop schedule and confirmed attendees forthcoming soon</em></p>

<h2 id="motivation">Motivation</h2>
<p>When RL is used in societally relevant domains, practitioners face a conundrum otherwise considered by law and policy communities: what is the domain (transportation, health care, social media) optimizing for? On the one hand, the technical language of RL provides a rich suite of methods for defining proxy rewards for particular tasks. But designers must then make substantive choices about what good behavior is allowed to mean or what terms and conditions apply. Furthermore, by their very nature, RL systems are dynamic, acting to shape their environment even as they learn from it. Together, these features lead to a looming question for the RLDM community: what would it mean to document the behavior of RL systems over time, both in relation to design choices and dynamic effects?</p>

<p>Building on the documentation frameworks for “model cards” and “datasheets” proposed by Mitchell et al. and Gebru et al., we have argued the need for Reward Reports for AI systems. In a whitepaper recently published by the Center for Long-Term Cybersecurity, we introduced Reward Reports as living documents for proposed RL deployments that demarcate design choices. However, many questions remain about the applicability of this framework to different RL applications, roadblocks to system interpretability, and the resonances between deployed supervised machine learning systems and the sequential decision-making utilized in RL. At a minimum, Reward Reports are an opportunity for RL practitioners to deliberate on these questions and begin the work of deciding how to resolve them in practice.</p>

<h2 id="format--the-unworkshop">Format – The (Un)Workshop</h2>
<p>We intend for this workshop to be a generative and interactive space for participants to try out and critique our Reward Reports proposal. As such, activities will be centered around doing and sharing in small groups, punctuated by periodic larger group discussions. We will begin with a round of introductions to create a friendly environment. After a presentation about the Reward Reports proposal by the organizers, participants will form small groups (2-3) based on target RL applications (real or imagined). Much of the workshop time will entail actually writing Reward Report-style documentation. Periodically, participants will form medium-sized groups based on application domain to discuss challenges or give feedback on drafts. Finally, we will conclude with a workshop-wide discussion on the practicality of the Reward Reports framework and elements that may be missing. We hope that by the end of the workshop, there will be two concrete outputs: a rich set of example RRs, and a list of refinements for the Reward Reports framework.</p>

<p>This interactive format does not include formal speakers; however, it will be important to have a diverse set of participants. While we hope many of the RLDM attendees will participate in the workshop, we have also extended invitations to RL researchers and practitioners as well as experts in AI documentation. Participants with confirmed interest include Elettra Bietti (NYU School of Law), Michael Dennis (Center for Human-Compatible AI), Alex Hertzberg (Cruise), Aaron Snoswell (Centre for Automated Decision-Making and Society), Elizabeth Anne Watkins (Center for Information Technology Policy), Meg Young (Fellow in the Mayor’s Office of the City of New York).</p>

<p>To learn more about Reward Reports, see the <a href="/assets/reward_reports_for_rl.pdf">Reward Reports paper</a>, the <a href="http://arxiv.org/abs/2202.05716">CLTC RL Risks Whitepaper</a>, or the <a href="https://github.com/RewardReports/reward-reports">github repo</a> with a template.</p>


    </div> <!-- container -->

  </div> <!-- page-content -->

  <nav class="navbar fixed-bottom navbar-light bg-light d-flex justify-content-center">
    <a class="navbar-brand" href="mailto:geese-org@lists.berkeley.edu">Contact Organizers</a>
  </nav>

</body>
</html>
